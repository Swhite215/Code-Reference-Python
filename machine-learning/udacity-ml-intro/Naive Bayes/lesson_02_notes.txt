Udacity Lesson 02 Naive Bayes

1. Acerous vs. Non-Acerous
    - Acerous - horns and antlers
    - Non-Acerous - lacking horns or antlers
    - Lesson - features are key characteristics

2. Supervised Classification Examples
    - Album of tagged photos, recognize someone
    - Given someone's music choice, recommend a new song - Recommender System

3. Features and Labels
    - Features
        - Intensity, Tempo, Genre, Gender
    - Labels
        - Like, Don't Like
    - Scatter Plot
        - Tempo + Intensity - Individual Songs
        - Green Songs - Like
        - Red Songs - Don't Like

4. Stanley Terrain Classification
    - Action - adjust speed
    - Features - steepness of terrain, ruggedness of terrain

5. Speed Scatterplot: Grade and Bumpiness
    - Features
        - Bumpiness - Smooth, Bad
        - Slope - Flat, Steep

6. From Scatterplot to Predictions
    - Important Question - what can I say about a new data point given the past data?

7. From Scatterplots to Descision Surfaces/Descision Bounaries
    - Surface - different sides are different classifications, separate classes so you can generalize new data points
    - Linear Descision Surface - straight line
    - Data -> Descision Surface

8. Transition to Using Naive Bayes
    - Navie Bayes - finding descision surface
    - NB Descision Boundary in Python
        - Goal - go fast vs. go slow

9. Geting Started With sklearn

10. Gaussian Naive Bayes Example
    - Important Lines
        - from sklearn.naive_bayes import GaussianNB - bring in external modules
        - clf = GaussianNB() - classifier equals GuassianNB() - create classifier
        - clf.fit(X,Y) - fit classifier by providing data to learn patterns - x = features, y = labels
        - print(clf.predict([X, Y])) - make prediction
    - Example Code - ClassifyNB.py
    - Documentation Link - https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html

11. Calculating NB Accuracy
    - Accuracy - number of test points that are classified correctly divided by the total number of test points
        - Take Predictions -> Compare to Lables
        - Use

12. Training and Testing Data
    - Training - 80-90%
    - Testing - 10-20%
    - Issue - overfit to training data, should be able to generalize new data

13. Unpacking NB Using Bayes Rule

14. Bayes Rule - Holy Grail of Probabilistic Inference
    - Cancer Test
        - P(C) = 0.01
        - Test - 90% if it is positive if you have C - sensitivity
        - Test - 90% negative if you don't have C - specitivity
        - Question - Positive Test - Probability you have cancer
        - Look at Positive Circle, then ratio of Cancerous to Entire Region