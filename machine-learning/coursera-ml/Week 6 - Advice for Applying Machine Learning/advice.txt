Machine Learning Advice

1. Evaluating a Learning Algorithm
    - Deciding What to Try Next
        - Situation - linear regression making error in predicting housing prices
        - Actions
            - Obtain more training examples
            - Try smaller set of features
            - Try getting additional features
            - Try adding polynomial features
            - Try decreasing lambda/regularization variable
            - Try increasing lambda/regularization variable
        - Special Technique - filters what options are worth pursuing
            - Machine Learning Diagnostic - test to run to gain insight into what is or isn't working with a learning algorithm
    - Evaluating a Hypothesis
        - Failure to Generalize/Overfitting Issue
            - Hard/Impossible to plot hypothesis
        - Method #1
            - Split data into two portions - Training Set and Testing Set 80%-20%, Randomize
            - If overfitting, expect training accuracy to be high and testing to be low
            - Procedure for Linear Regression
                - Learn parameters theta from training set of 70-80%
                - Compute test set error, take learned theta and plug into test set cost function
            - Procedure for Logistic Regression
                - Learn parameters theta from training set of 70-80%
                - Compute test set error, take learned theta and plug into test set cost function
                - Misclassification error (0/1 missclassification error)
                    - if h(x) >= 0.5 and y = 0 - ERROR
                    - if h(x) < 0.5 and y = 1 - ERROR
                    - 0 if correct
        - IMPORTANT - performance on training set doesnt necessarily correctly estimate with new data
    - Model Selection and Train/Validation/Test Sets
        - Model Selection Problem - linear, quadratic, cubic, higher order polynomial
            - Issue Path
                - Use different models, generate theta, pass theta to test set to determine test set error
                - Issue - performance on hypothesis to test set is bias as we selected the order based on test set
            - Best Action for Model Selection
                - Split dataset into three groups - training, cross validation, test set - 60%-20%-20%
                - Train Error, Cross Validation Error, Test Error
                - Procedure
                    - Take 1st hypothesis and minimize cost function to return parameter theta
                    - Take nth hypothesis and minimize cost function to return parameter theta
                    - Test hypothesis on cross validation set to see how well each hypothesis does
                    - Pick hypothesis with lowest cross validation error to determine model and degree
                    - Estimate the generalization error using the theta that performed the best on the cross validation set

2. Bias vs. Variance
    - Diagnosing Bias vs. Variance
        - Definitions
            - Bias - underfitting the data
            - Just Right
            - Variance - overfitting the data
            - Training Error - should decrease as polynomial increases
            - Cross Validation Error - high low degree and high high degree, just right with mid degree
        - Is it Bias vs. Variance?
            - Cross Validation High w/ Low Order = High Bias Problem
                - Both cross validation and training error are going to be high
            - Cross Validation High w/ High Order = High Variance Problem
                - Cross validation error is high and training set error are low
    - Regularization and Bias/Variance
        - Large Lambda - High Bias - Underfit
        - Small Lambda - High Variance - Overfit
        - Action
            - Try Lambda at 0, .01, .02, .04, .08 ... 10.24s (12)
            - Take 1st Model with Lambda at 0 -> Minimize Cost Function J(Theta)
            - Take 2nd Model with Lambda at .01 -> Minimize Cost Function J(Theta)
            - Take Nth Model with Lambda at Value -> Minimize Cost Function J(Theta)
            - For every hypothesis -> Send to cross validation set to find cv error
            - Pick whichever model gives lowest error on cv set
            - Take model and look at how well it does on test set
        - Bias/Variance as a Function of Regularization Parameter Lambda
            - JTrain Theta - increase as lambda increases i.e. high bias as lambda increases
            - Cross Validation Error - large lambda underfit cross validation set, small lambda overfit cross validation set
            - Goal - low cross validation error
    - Learning Curves
        - Action
            - Plot JTrain - average square error on training set
            - Plot JCV - average square error on cross validation set
            - Plot training set size (x) by error (y), reduce training set size
            - m = 1,2,3 error on training set ~0
            - As training set size grows, average training set error grows
            - As training set size grows, average cross validation set error decreases
            - High Bias - as training set size grows, cross validation error decreases, training error increases, cv and training approach each other, high error
            - Insight - if a learning algorithm is suffering from high bias, getting more training data will not by itself help much
            - High Variance - as training set size grows, train error increases, cross validation error decreases but remains high
            - Insight - if a learning algorithm is suffering from high variance, getting more training data is likely to help
    - Deciding What to Do Next
        - Issue - Regularized Linear Regression fails to predict
        - Actions
            - Get more training examples - useful if high variance (cross validation error > training set error)
            - Try smaller set of features - useful if high variance
            - Try getting additional features - use if high bias
            - Try adding polynomial features - use if high bias
            - Try decreasing lambda - fixes high bias
            - Try increasing lambda - fixes high variance
        - Relating to Neural Networks
            - Underfitting - small neural network, small/few hidden layers, fewer parameters, more prone to underfitting
            - Overfitting - large nueral network, large/many hidden layers, more parameters, more prone to overfitting

3. Building a Spam Classifier
    - Prioritizing What to Work On
        - Supervised Learning - x = features of email, y = spam(1) or not spam(0), choose 100 words indicative of spam/not spam
        - Convert email to feature vector by taking feature words and creating vector that indicates whether feature word appears in the email
        - How to spend your time to make it have low error?
            - Collect lots of data - not necessarily all the time
            - Develop sophisticated features based on email routing information
            - Develop sophisticated features for message body
            - Develop sophisticated algorithms to detect misspellings
    - Error Analysis
        - Start with a simple algorithm you can implement quickly, implement and test it on cross validation set
        - Plot learning curves to decide if more data, more features, etc are likely to help
        - Error Analysis - manually examine the examples in cross validation set that your algorithm made errors on. See if you spot any systematic trend in what type of examples it is making errors on
            - 500 Examples, Algorithm Misclassifies 100
                - Manually examine, what type of email it is
                - What features you think would have helped the algorithm classify them correctly
        - Ensure you have numerical evaluation of learning algorithm

4. Handling Skewed Data
    - Skewed Classes - more examples from one class than the other class
    - Precision/Recall
        - True Positive - Actual = 1 and Predicted = 1
        - False Positive - Actual = 0 and Predicted = 1
        - False Negative - Actual = 1 and Predicted = 0
        - True Negative - Actual = 0 and Predicted = 0
        - Precision - True Positives / Predicted Positives (Sum of True Positive and False Positive) - High Precision means low false positives, close to 1
        - Recall - True Positives / Actual Positives (Sum of True Positives and False Negatives)
        - Using Precision and Recall - High both suggests algorithm is doing well even if we have skewed classes
    - Trading Precision and Recall
        - Improve confidence by changing threshold of classification - high precision, lower recall, higher fraction will turn out to have cancer
        - Decrease confidence by changing threshold of classification - lower precision, higher recall, higher fraction will turn out not to have cancer
        - High Precision Low Recall - high threshold = high confidence
        - Low Precision High Recall - low threshold = low confidence
        - How to compare precision/recall numbers?
            - Algorithm 1 - P = 0.5 and R = 0.4
            - Algorithm 2 - P = 0.7 and R = 0.1
            - Algorithm 3 - P = 0.02 and R = 1.0
            - Options
                - Look at highest averge value - could end up with extremes - not great choice
                - F1 Score - 2 (PR)/(P+R)

5. Using Large Data Sets
    - Data for Machine Learning
        - How much data to train on?
        - Story
            - Designing a High Accuracy Learning System
                - Algorithms vs. Data Sizes
                - Problem - classifying between confusable words e.g. (to, two, too) and (then, than)
                - Algorithms
                    - Perceptron (Linear Regression)
                    - Winnow
                    - Memory-Based
                    - Naive Bayes
                - Action
                    - Varied training set size and tried out each algorithm
                - Insight 
                    - Most algorithms gave similar peformance
                    - As training set size increases, accuracy tends to increase
                    - "Its not who has the best algorithmn that wins, its who has the most data"
                - Assumptions
                    - Large Data Rationale
                        - Assume features x has sufficient information to predict y accurately
                        - Useful test: given the input x, can a human expert confidently predict y?
                        - Training Error Small, Use Large Training Set with Many Parameters (Features or Hidden Units), Test Error Small, Low Bias, Low Variance