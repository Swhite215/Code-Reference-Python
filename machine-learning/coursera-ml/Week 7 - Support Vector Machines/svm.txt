Support Vector Machines

1. Large Margin Classification
    -  Alternate View of Logistic Regression
        - Hypothesis -> Sigmoid Function
            - y = 1, hypothesis ~ 1, z > 0
            - y = 0, hypothesis ~ 0, z < 0
        - Cost Function
            - y = 1, only first portion of cost function matters, z is large, small value of cost function
            - y = 0, only second portion of cost function matters, z is small, small value of cost function
            - Cost₁(z) and Cost₀(z)
        - Support Vector Machine - Take Logistic Regression, replace with Cost₁(z) and Cost₀(z) functions
            - Removing 1/m, doesn't change what minimizes the function
            - Switch from A + Lambda B to CA + B, C = 1/Lambda
        - Support Vector Machine Hypothesis - Outputs - 1 or 0
    - Large Margin Intuition
        - Cost₁(z) is >= 1, then y = 1
        - Cost₀(z) is <= -1, then y = 0
        - Imagine we set C to a large value e.g. 100,000
            - When minimizing, will focus on getting first term equal to zero
            - Minimize Cx) + 1.2 sum(theta squared j)
        - Decision Boundary - Linearly Separable Case
            - Line has larger and more equal margin between positive and negative classes
        - C Intuition
            - When large, margin is smaller
            - When small, margin is larger
    - Mathematics Behind Large Margin Classification
        - Objective - setting of parameters where the norm of theta is small

2. Kernels
    - Kernels I
        - Kernel - technique for developing complex nonlinear classifiers using SVM
            - Choose Landmark, Use for Similarity Function (Kernel i.e. Gaussian Kernel)
            - If x is closed to landmark, feature will be close to 1
            - If x is far from landmark, feature will b eclose to 0
            - As sigma squared decreases, feature drops to zero quickly
            - As sigma squared increases, feature drop to zero more slowly
            - Compute Features
                - Hypothesis is 1 when features with theta >= 0
                - Hypothesis is 0 when feature with theta <= 0
    - Kernels II
        - Where do we get landmarks from?
            - Given a machine learning problem with some data set, for every training example (Xi, Yi), replace with landmark, ending with m landmarks
            - Given an example x, compute feature vector, feature is similairty between Xi and Li
            - Predict y = 1 if Thetaᵀf >= 0
            - Training - Minimize Cost Function using Thetaᵀf
        - Bias and Variance Trade Off
            - Must choose parameter C
                - Large C: Lower Bias and Higher Variance
                - Small C: Higher Bias and Lower Variance
            - Must Choose Sigma
                - Large Sigma - Features vary more slowthy, so higher bias and lower variance
                - Small Sigma - Features vary less smoothly, so lower bias and higher variance

3. SVMs in Practice
    - Using a SVM
        - Use SVM Software Package - liblinear, libsvm, ... to solve for parameters theta
        - Choice of Parameter C
        - Choice of Kernel
            - No Kernel - linear kernel, predict y = 1, n large features and m small number of examples
            - Gaussian Kernel - n small number of features and m large training examples, fit more complex non linear boundary
                - Need to choose sigma^2
                - Implement similarity function to compute feature value
                - Perform feature scaling before using Gaussian Kernel
            - Polynomial Kernel
            - String Kernel
            - Chi-Square Kernel
            - Histogram Intersection Kernel
        - Multi-Class Classification
            - Train K SVMs, one to distinguish y = i from the rest, predict the class i with the lragest theta transpose x
        - Logistic Regression vs. SVMs
            - If n is large, n >= m, n = 10,000 and m = 1 ... 1000, then use logistic regression or SVM without a kernel (linear kernel)
            - If n is small and m is intermediate, n = 1 ... 1000, and m = 10 ... 10,000, then use a SVM with Gaussian Kernel
            - If n is small and m is large, Create/add more features, then use logistic regression or SVM without a kernel
        - Neural Network - likely to work well, but may be slower to train